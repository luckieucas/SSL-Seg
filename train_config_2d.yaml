exp: "img_size448" # threshhold for model1 and model2 are 0.95
method: CTCT # use which semi-supervised method [Baseline,MT,EM,C3PS,DAN,CPS,UAMT,ConNet,McNet,CVCL,CSSR,CVCL_partial]
train_3D: False # whether to train 3D models
backbone: unet #[unet_3D_old, McNet,unet_3D_cl, unet ]# backbone model
backbone2: unet_3D_sr #unet_3D_condtion_decoder # backbone for second network [unet_3D_condtion_decoder,unet_3D_condtion,Unet3DConditionBottom]
dataset_name: BCV
gpu: '1'
FP16: False
root_path: ../../dataset/Task11_BCV/Training

# continue training setting
continue_wandb: False
continue_training: False
wandb_id: "2jkdqxmi"
model_checkpoint: "../model/BCV_500_CVCL_partial_test_SGD_SGD/resnet_3D_cvcl/model_iter_11200_dice_0.7703.pth"
model2_checkpoint: "../model/BCV_4_CSSR_new_large_patch_size_SGD_SGD/unet_3D_old/model2_iter_4200_dice_0.6676.pth"

current_iter_num: 11200

# semi supervised learning setting
began_semi_iter: 8000  #8000 #2000
began_condition_iter: 6000 #4000

# evaluation settings
began_eval_iter: 100 # 300
val_freq: 200 #200

# training settings
max_iterations: 60000
use_CAC: True   # whether use Context-Aware-Consistency
use_PL: False # whether use partial label
deterministic: 1
initial_lr: 0.01
initial2_lr: 0.01 # for second network
optimizer_type: 'SGD' # ['SGD' or 'Adam']
optimizer2_type: 'SGD' # for second network
model1_thresh: 0.90 #threshhold for model1
model2_thresh: 0.95 # threshhold for model2
seed: 1337
ema_decay: 0.99
consistency_type: mse
consistency: 0.1 #0.1 1.0 for McNet
consistency_rampup: 200.0
weight_decay: 0.00001 #0.00001
lr_scheduler_patience: 30
lr_scheduler_eps: 0.001
show_img_freq: 100
save_checkpoint_freq: 3000

METHOD:
  ICT:
    ict_alpha: 0.2
  MT:
  UAMT:
  CPS:
  DAN:
  EM:
  Baseline:
  URPC:
  C3PS: # config for Context-Aware-Consistency
    stride: 2
    iou_bound_low: 0.1
    iou_bound_high: 0.9
    con_list: [1,2,3,4,5,6,7] # specificy condition list
    addition_con_list: [8]
  ConNet: # config for Context-Aware-Consistency
    stride: 2
    iou_bound_low: 0.1
    iou_bound_high: 0.9
    con_list: [1,2,3,4,5,6,7] # specificy condition list
    addition_con_list: [8]
  McNet:
  CVCL:
    stride: 8
    step_save: 2
    iou_bound_low: 0.1
    iou_bound_high: 0.9
    con_list: [1,2,3,4,5,6,7] # specificy condition list
    addition_con_list: [8]
    threshold: 0.5
  CVCL_partial:
    stride: 8
    step_save: 2
    iou_bound_low: 0.1
    iou_bound_high: 0.9
    con_list: [1,2,3,4,5] # specificy condition list
    addition_con_list: [8]
    threshold: 0.5
    began_partial: 4000
  CSSR: # config for Context-Aware-Consistency
    stride: 2
    iou_bound_low: 0.1
    iou_bound_high: 1.01
    patch_size_large: [128,208,288] #[144, 256, 320]
  CTCT:
    cache_mode: part
    zip: True
    accumulation_steps:
    use_checkpoint: False 
    amp_opt_level: O1
    tag:
    eval: False 
    throughput: False


DATASET:
  labeled_num: 4
  labeled_num_pl: 60 # labeled number for partial labeled data
  labeled_bs: 8 #1
  batch_size: 16 #2
  patch_size: [224,224] # [448,448] #[224, 224] #[96,112,112] # for DAN #[80,112,112] #[96,160,160]
  cutout: True # do random cutout
  rotate_trans: True # do random rotate
  scale_trans: True # do random scale
  random_rotflip: False # do random rotation and flip
  edge_prob: 0.1 # prob to learn edge slice
  normalization: 'Zscore'
  LA: # config for LA dataset
    num_classes: 2
    class_name_list: ['LA']
    weights: [1.0]
    train_list: "../data/LA/LA_train.txt"
    train_list_pl: "../data/LA/LA_test.txt" # train list for partial labeled data
    test_list: "../data/LA/LA_test.txt"
    training_data_num: 80
    testing_data_num: 20
    cut_upper: 300 # 1000  200 for urpc
    cut_lower: -200 # -1000 -68 for urpc 
  MMWHS: # config for MMWHS dataset
    num_classes: 8
    class_name_list: ['MYO', 'LA', 'LV', 'RA', 'AA', 'PA', 'RV','Foreground']
    weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
    train_list: "../data/MMWHS/train.list" #"../data/MMWHS/MMWHS_train.txt"
    train_list_pl: "../data/Pancreas/pancreas_test.txt" # train list for partial labeled data
    test_list: "../data/MMWHS/test.list" #"../data/MMWHS/MMWHS_test.txt"
    training_data_num: 14
    testing_data_num: 6
    cut_upper: 300 # 1000  200 for urpc
    cut_lower: -200 # -1000 -68 for urpc
  MMWHS_MR: # config for MMWHS dataset
    num_classes: 8
    train_list: "../data/MMWHS_MR/train.txt"
    test_list: "../data/MMWHS_MR/test.txt"
    training_data_num: 14
    testing_data_num: 6
    cut_upper: 1000
    cut_lower: -1000
  BCV: # config for BCV dataset
    num_classes: 6
    class_name_list: ['Spleen', 'Right_Kidney', 'Left_Kidney','Liver','Pancreas','Foreground']
    weights: [1.0,1.0,1.0,1.0,1.0] #[0.2,0.2,0.2,0.1,0.3]
    train_list: "../data/BCV/train.txt"
    train_list_pl: "../data/Pancreas/pancreas_test.txt" # train list for partial labeled data
    test_list: "../data/BCV/test.txt"
    training_data_num: 24
    testing_data_num: 6 
    cut_upper: 275 #200
    cut_lower: -125 #-68
  FLARE: # config for BCV dataset
    num_classes: 14
    train_list: "../data/FLARE22/Flare22_train.txt"
    test_list: "../data/FLARE22/Flare22_test.txt"
    training_data_num: 2038
    testing_data_num: 10 



model:
  # model class, e.g. UNet3D, ResidualUNet3D
  name: Semi3DUNet
  # number of input channels to the model
  in_channels: 1
  # number of output channels
  out_channels: 8
  # determines the order of operators in a single layer (gcr - GroupNorm+Conv3d+ReLU)
  layer_order: gcr
  # feature maps scale factor
  f_maps: 16 #16
  # number of levels
  num_levels: 5 #4
  # number of groups in the groupnorm
  num_groups: 8
  # apply element-wise nn.Sigmoid after the final 1x1 convolution, otherwise apply nn.Softmax
  final_sigmoid: False
  # if True applies the final normalization layer (sigmoid or softmax), otherwise the networks returns the output from the final convolution layer; use False for regression problems, e.g. de-noising
  is_segmentation: true